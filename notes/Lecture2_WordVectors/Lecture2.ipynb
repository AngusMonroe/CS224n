{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture2 Word Vectors\n",
    "\n",
    "### 如何在计算机中使用词意\n",
    "\n",
    "通用解决方案：使用例如WordNet,一个包含列表的**同义词集**和**上位词集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n)\n",
      "good\n",
      "(n)\n",
      "good, goodness\n",
      "(n)\n",
      "good, goodness\n",
      "(n)\n",
      "commodity, trade_good, good\n",
      "(a)\n",
      "good\n",
      "(s)\n",
      "full, good\n",
      "(a)\n",
      "good\n",
      "(s)\n",
      "estimable, good, honorable, respectable\n",
      "(s)\n",
      "beneficial, good\n",
      "(s)\n",
      "good\n",
      "(s)\n",
      "good, just, upright\n",
      "(s)\n",
      "adept, expert, good, practiced, proficient, skillful, skilful\n",
      "(s)\n",
      "good\n",
      "(s)\n",
      "dear, good, near\n",
      "(s)\n",
      "dependable, good, safe, secure\n",
      "(s)\n",
      "good, right, ripe\n",
      "(s)\n",
      "good, well\n",
      "(s)\n",
      "effective, good, in_effect, in_force\n",
      "(s)\n",
      "good\n",
      "(s)\n",
      "good, serious\n",
      "(s)\n",
      "good, sound\n",
      "(s)\n",
      "good, salutary\n",
      "(s)\n",
      "good, honest\n",
      "(s)\n",
      "good, undecomposed, unspoiled, unspoilt\n",
      "(s)\n",
      "good\n",
      "(r)\n",
      "well, good\n",
      "(r)\n",
      "thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"(%s)\" % synset.pos())\n",
    "    print(\", \".join([l.name() for l in synset.lemmas()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-658572dd9443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpanda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"panda.n.01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpanda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'closure'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synsets(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet存在的问题\n",
    "\n",
    "- 作为资源很好，但缺乏细微差别\n",
    "\n",
    "\t- “proficient”与“good”是同义词，但这只在某些情况下是正确的\n",
    "\n",
    "- 缺少词语的新含义\n",
    "\n",
    "\t- 例如 wicked, badass, nifty, wizard, genius, ninja, bombest\n",
    "\t- 不可能持续更新\n",
    "\n",
    "- 主观\n",
    "- 需要人工创建和适应\n",
    "- 很难计算词语的相似性\n",
    "\n",
    "### 将单词表示为离散符号\n",
    "\n",
    "在传统的NLP中，我们将单词视为离散的符号\n",
    "\n",
    "词语可以被表示为one-hot向量（向量中有一维是1，其余为0），向量维数即为词典中词语个数\n",
    "\n",
    "### 将单词作为离散符号的问题\n",
    "\n",
    "在网络搜索中，如果用户搜索“Seattle motel”，我们希望检索结果包含““Seattle hotel”\n",
    "\n",
    "但：\n",
    "\n",
    "> motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
    "> hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
    "\n",
    "这两个向量是正交的，one-hot向量没有自然的**相似性**概念，所以我们要尝试在向量本身中编码相似性\n",
    "\n",
    "### 根据上下文表示单词\n",
    "\n",
    "- 核心思想：**一个词的含义是由经常出现在这个词附近的词给出的**\n",
    "\n",
    "- 当一个词出现在文章中时，其上下文就是出现在它附近的词（在一定范围内）\n",
    "\n",
    "- 使用w的许多上下文来构建w的表示\n",
    "\n",
    "    ![JPEG 图像-9FDC3ECCBDAB-1.jpeg](https://i.loli.net/2018/04/09/5acacb2c8585d.jpeg)\n",
    "    \n",
    "### 词向量\n",
    "\n",
    "为每一个单词建立一个密集的向量选择与其出现在类似语境下的向量\n",
    "\n",
    "Note: word vectors are sometimes called word embeddings or word representations.\n",
    "\n",
    "### Word2vec：概述\n",
    "\n",
    "Word2vec（Mikolov et al。2013）是一个词向量的学习框架\n",
    "\n",
    "理念：\n",
    "- 我们有一个大的文本语料库\n",
    "- 固定词汇表中的每个词都由一个向量表示\n",
    "- 通过文本中的每个位置t，它有一个中心词c和上下文（“外部”）字o\n",
    "- 使用c和o的单词向量的相似度来计算给定c的概率（反之亦然）\n",
    "- 继续调整单词向量以最大化这个概率\n",
    "\n",
    "计算$P(w_{t+j}|w_{t})$\n",
    "\n",
    "![屏幕快照 2018-04-09 上午10.13.26.png](https://i.loli.net/2018/04/09/5acacc784f30d.png)\n",
    "\n",
    "![屏幕快照 2018-04-09 上午10.47.55.png](https://i.loli.net/2018/04/09/5acad471f35bf.png)\n",
    "\n",
    "### Word2vec：目标函数\n",
    "\n",
    "对于每一个位置t=1,...,T，，预测固定大小为m的窗口内的上下文单词，得到中心词$w_{j}$。\n",
    "\n",
    "![屏幕快照 2018-04-09 上午10.49.25.png](https://i.loli.net/2018/04/09/5acad4c97f473.png)\n",
    "\n",
    "目标函数（有时称为成本函数损失函数）⌡(θ)是（平均）负对数似然性：\n",
    "\n",
    "![屏幕快照 2018-04-09 上午11.00.16.png](https://i.loli.net/2018/04/09/5acad7501c623.png)\n",
    "\n",
    "最小化目标函数⟺最大化预测准确性\n",
    "\n",
    "我们想要最小化目标函数⌡(θ)，为了计算$P(w_{t+j}|w_{t};θ)$，我们将用两个向量表示一个词w：\n",
    "\n",
    "- 当w是中心词时用$v_{w}$\n",
    "- 当w是上下文词时用$u_{w}$\n",
    "\n",
    "然后对于中心词c和上下文词o：\n",
    "\n",
    "![屏幕快照 2018-04-09 上午11.06.28.png](https://i.loli.net/2018/04/09/5acad8c14e526.png)\n",
    "\n",
    "### Word2vec：预测功能\n",
    "![屏幕快照 2018-04-09 下午3.17.25.png](https://i.loli.net/2018/04/09/5acb139010a77.png)\n",
    "\n",
    "点积比较o和c的相似性。较大的点积=较大的概率\n",
    "\n",
    "取指数后，对整个词汇表进行归一化\n",
    "\n",
    "Softmax function：从实数空间到概率分布的标准映射方法\n",
    "\n",
    "![屏幕快照 2018-04-09 下午3.20.03.png](https://i.loli.net/2018/04/09/5acb1432365d7.png)\n",
    "\n",
    "softmax函数将任意值$x_{i}$映射到概率分布$p_{i}$，在深度学习中使用很频繁\n",
    "\n",
    "指数函数可以把实数映射成正数，然后归一化得到概率。softmax之所叫softmax，是因为指数函数会导致较大的数变得更大，小数变得微不足道；这种选择作用类似于max函数。\n",
    "\n",
    "### 训练模型：计算所有矢量梯度\n",
    "\n",
    "把所有参数写进向量θ，对d维的词向量和大小V的词表来讲，有：\n",
    "\n",
    "![屏幕快照 2018-04-09 下午3.38.08.png](https://i.loli.net/2018/04/09/5acb18673af45.png)\n",
    "\n",
    "每个单词都有两个向量\n",
    "\n",
    "然后我们优化这些参数\n",
    "\n",
    "### 梯度的推导\n",
    "\n",
    "具体推导方法见[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html#h3-5)\n",
    "\n",
    "### 损失/目标函数\n",
    "\n",
    "梯度有了，参数减去梯度就能朝着最小值走了\n",
    "\n",
    "![屏幕快照 2018-04-09 下午5.36.23.png](https://i.loli.net/2018/04/09/5acb3421da2b2.png)\n",
    "\n",
    "只有一句比较新鲜，神经网络喜欢嘈杂的算法，这可能是随机梯度下降(SGD)成功的另一原因\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
