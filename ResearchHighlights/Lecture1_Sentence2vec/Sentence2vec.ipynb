{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture1 Sentence2vec\n",
    "\n",
    "这次的research highlight是Princeton的一篇论文，主要内容是一个简单但很难超越的Sentence Embedding基线方法\n",
    "\n",
    "在神经网络泛滥的时候，这篇文章像一股清流，提出了一个无监督的句子建模方法，并且给出了该方法的一些理论解释。通过该方法得到的句子向量，在STS数据集上取得了不输给神经网络的效果。\n",
    "\n",
    "![1.jpg](https://i.loli.net/2018/04/10/5acc07189eebd.jpg)\n",
    "\n",
    "### 句子Embedding动机\n",
    "\n",
    "第二节课一直在讲词向量编码词的意思，但自然语言处理真正关心的是整个句子的意思。\n",
    "\n",
    "如果我们能够拿到句子的向量表示，则可以方便的用內积计算相似度：\n",
    "\n",
    "![2.jpg](https://i.loli.net/2018/04/10/5acc07c88995e.jpg)\n",
    "\n",
    "还可以在这些句子向量之上构建分类器做情感分析：\n",
    "\n",
    "![3.jpg](https://i.loli.net/2018/04/10/5acc07fabe878.jpg)\n",
    "\n",
    "### 已有方法\n",
    "\n",
    "具体怎么由词向量到句向量呢？有很多种方法，比如词袋模型中简单地线性运算：\n",
    "\n",
    "![4.jpg](https://i.loli.net/2018/04/10/5acc0840e4102.jpg)\n",
    "\n",
    "后面的课程中，将会用recurrent neural network、recursive neural network，CNN来做同样的事情。\n",
    "\n",
    "![5.jpg](https://i.loli.net/2018/04/10/5acc08af83599.jpg)\n",
    "\n",
    "### A Simple but Tough-to-beat Baseline for Sentence Embeddings\n",
    "\n",
    "但今天要介绍的这篇普林斯顿大学的论文却剑走偏锋，采用了一种简单的无监督方法。这种方法简单到只有两步：\n",
    "\n",
    "1. 对句子中的每个词向量，乘以一个独特的权值。这个权值是一个常数α除以α与该词语频率的和，也就是说高频词的权值会相对下降。求和后得到暂时的句向量s。\n",
    "2. 计算语料库所有句向量构成的矩阵的第一个主成分u，让每个句向量减去它在u上的投影（类似PCA）。其中，一个向量v在另一个向量u上的投影定义如下：\n",
    "\n",
    "\t![屏幕快照 2018-04-10 上午8.46.10.png](https://i.loli.net/2018/04/10/5acc0962265af.png)\n",
    "\t\n",
    "![6.jpg](https://i.loli.net/2018/04/10/5acc098c1c106.jpg)\n",
    "\n",
    "### 句子建模算法\n",
    "\n",
    "作者将该算法称之为WR，W表示Weighted，意为使用预估计的参数给句中的每个词向量赋予权重，R表示Removal，意为使用PCA或者SVD方法移除句向量中的无关部分。\n",
    "\n",
    "![9.png](https://i.loli.net/2018/04/10/5acc11322a8a6.png)\n",
    "\n",
    "输入： \n",
    "- 预训练的词向量{$v_{w}$:w∈V}，例如word2vec、glove等 \n",
    "- 待处理的句子集合 S\n",
    "- 参数a（论文中建议a的范围：[1e−4,1e−3]） \n",
    "- 词频估计{p(w):w∈V}\n",
    "\n",
    "输出： \n",
    "- 句子向量{$v_{s}$:s∈S}\n",
    "\n",
    "\n",
    "### 概率论解释\n",
    "\n",
    "其原理是，给定上下文向量，一个词的出现概率由两项决定：作为平滑项的词频，以及上下文：\n",
    "\n",
    "![7.jpg](https://i.loli.net/2018/04/10/5acc09f15faa2.jpg)\n",
    "\n",
    "其中第二项的意思是，有一个平滑变动的上下文随机地发射单词。\n",
    "\n",
    "### 效果\n",
    "\n",
    "在句子相似度任务上超过平均水平，甚至超过部分复杂的模型。在句子分类上效果也很明显，甚至是最好成绩。\n",
    "\n",
    "![8.jpg](https://i.loli.net/2018/04/10/5acc0a278ce29.jpg)\n",
    "\n",
    "这是文中的第一个实验——句子相似性评价。 \n",
    "实验使用的数据集都是公共数据集，在这些数据集上方法都取得了不输给RNN和LSTM的表现。\n",
    "\n",
    "但是在情感分析方面，该方法不及RNN和LSTM，作者分析的了可能原因： \n",
    "1. 算法使用的词向量(word2vec, glove等)大都基于分布式假说——拥有相近上下文的单词具有相近的意思，但是这些词向量对句子中的antonym problem(我的理解是句子中会出现转折)的感知能力有限。 \n",
    "2. 对于预估计词频来确定权重的方式，在情感分析中可能不是很有效。例如，单词”not”在情感分析中是非常重要的，但是在确定权重时，采用的词频估计会导致其难以在情感分析中发挥作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验复现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "# import PSLvec as psl\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "# glove_path = './glove_model.txt'\n",
    "# psl_path = './PSL_model.txt'\n",
    "# traindata = './datasets/sts2013.OnWN.pkl'\n",
    "# freq_table = './mydictionary'\n",
    "embedding_size = 300\n",
    "\n",
    "# pslemb = psl.PSL()\n",
    "\n",
    "# 载入word2vec模型\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_path,binary=True)\n",
    "# model = KeyedVectors.load_word2vec_format(glove_path,binary=False)\n",
    "# model = KeyedVectors.load_word2vec_format(psl_path,binary=False)\n",
    "# model = pslemb.w\n",
    "print('完成模型载入')\n",
    "\n",
    "# tokenizer = StanfordTokenizer(path_to_jar=r\"D:\\stanford-parser-full-2016-10-31\\stanford-parser.jar\")\n",
    "\n",
    "\n",
    "# print(type(model))\n",
    "# print(model['sdfsfsdfsadfs'])\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, text, vector):\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return len(self.word_list)\n",
    "\n",
    "\n",
    "def get_word_frequency(word_text, looktable):\n",
    "    if word_text in looktable:\n",
    "        return looktable[word_text]\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def sentence_to_vec(sentence_list: List[Sentence], embedding_size, looktable, a=1e-3):\n",
    "    sentence_set = []\n",
    "    for sentence in sentence_list:\n",
    "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = sentence.len()\n",
    "        for word in sentence.word_list:\n",
    "            a_value = a / (a + get_word_frequency(word.text, looktable))  # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word.vector))  # vs += sif * word_vector\n",
    "\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "\n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA(n_components=embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "    return sentence_vecs\n",
    "\n",
    "\n",
    "# with open(freq_table, 'rb') as f:\n",
    "#     mydict = pkl.load(f)\n",
    "# print('完成词频字典载入')\n",
    "\n",
    "paths = ['./datasets/data']\n",
    "for path in paths:\n",
    "    files = []\n",
    "    for file in os.listdir(path=path):\n",
    "        if os.path.isfile(path + '/' + file):\n",
    "            files.append(path + '/' + file)\n",
    "\n",
    "    for traindata in files:\n",
    "        with open(traindata, 'rb') as f:\n",
    "            train = pkl.load(f)\n",
    "\n",
    "        print('读取' + traindata + '数据完成')\n",
    "\n",
    "        gs = []\n",
    "        pred = []\n",
    "        allsent = []\n",
    "        for each in train:\n",
    "            # sent1, sent2, label = each.split('\\t')\n",
    "            if len(train[0]) == 3:\n",
    "                sent1, sent2, label = each\n",
    "            else:\n",
    "                sent1, sent2, label, _ = each\n",
    "            gs.append(float(label))\n",
    "            s1 = []\n",
    "            s2 = []\n",
    "            # sw1 = sent1.split()\n",
    "            # sw2 = sent2.split()\n",
    "            for word in sent1:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s1.append(Word(word, vec))\n",
    "            for word in sent2:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s2.append(Word(word, vec))\n",
    "\n",
    "            ss1 = Sentence(s1)\n",
    "            ss2 = Sentence(s2)\n",
    "            allsent.append(ss1)\n",
    "            allsent.append(ss2)\n",
    "\n",
    "        # sentence_vectors = sentence_to_vec(allsent, embedding_size, looktable=mydict)\n",
    "        sentence_vectors = sentence_to_vec(allsent, embedding_size)\n",
    "        len_sentences = len(sentence_vectors)\n",
    "        for i in range(len_sentences):\n",
    "            if i % 2 == 0:\n",
    "                sim = cosine_similarity([sentence_vectors[i]], [sentence_vectors[i + 1]])\n",
    "                pred.append(sim[0][0])\n",
    "\n",
    "        print('len of pred: ', len(pred))\n",
    "        print('len of gs: ', len(gs))\n",
    "\n",
    "        r, p = pearsonr(pred, gs)\n",
    "        print(traindata + '皮尔逊相关系数:', r)\n",
    "\n",
    "\n",
    "        # sentence_vectors = sentence_to_vec([ss1, ss2], embedding_size, looktable=mydict)\n",
    "        # sim = cosine_similarity([sentence_vectors[0]], [sentence_vectors[1]])\n",
    "        # pred.append(sim[0][0])\n",
    "\n",
    "        # r, p = pearsonr(pred, gs)\n",
    "        # print(traindata + '皮尔逊相关系数:', r)  # print(sentence_vectors[0])\n",
    "# print(sentence_vectors[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "这种句子的建模方式非常高效且便捷。由于这是一种无监督学习，那么就可以对大规模的语料加以利用，这是该方法相比于一般有监督学习的一大优势。 \n",
    "\n",
    "通过对实验的复现，发现运行一次程序只需要十几分钟，并且主要的运行耗时都在将词向量模型载入内存这个过程中，这比动不动就需要训练几周的神经网络模型确实要好很多，并且在这个词相似性任务中，与神经网络旗鼓相当。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
