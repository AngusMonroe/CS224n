{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3 Efficient Text Classification\n",
    "\n",
    "## Facebook的fastText \n",
    "\n",
    "\n",
    "文本分类是NLP中常见的任务，比如情感分析：\n",
    "\n",
    "![006Fmjmcly1fggfamp65aj30nj0bsjtd.jpg](https://i.loli.net/2018/07/17/5b4d9d2fb36cf.jpg)\n",
    "\n",
    "### 词袋模型\n",
    "\n",
    "虽然词袋模型只是所有词向量的某种平均，但其维度可以做到很低：\n",
    "\n",
    "![006Fmjmcly1fggfc58nk5j30pe0dnacc.jpg](https://i.loli.net/2018/07/17/5b4d9d304fecd.jpg)\n",
    "\n",
    "为了抵抗词序丢失带来的语义丢失问题，可以用ngram特征来代替。\n",
    "\n",
    "### 简单的线性模型\n",
    "\n",
    "这并不是神经网络，因为从输入到隐藏层只是一个look-up table，而隐藏层到输出则是一个逻辑斯谛回归线性分类器。\n",
    "\n",
    "![006Fmjmcly1fggfg4an8uj30os0d4dgk.jpg](https://i.loli.net/2018/07/17/5b4d9d301ba88.jpg)\n",
    "\n",
    "### 训练\n",
    "\n",
    "用交叉熵作为损失函数：\n",
    "\n",
    "![006Fmjmcly1fggfgyp14aj30lh0au75r.jpg](https://i.loli.net/2018/07/17/5b4d9d307890b.jpg)\n",
    "\n",
    "### Hierarchical softmax\n",
    "\n",
    "与其用一个超大的softmax层，不如用多个Hierarchical softmax：\n",
    "\n",
    "![006Fmjmcly1fggfinvdyxj30ox0eljtw.jpg](https://i.loli.net/2018/07/17/5b4d9d30a8424.jpg)\n",
    "\n",
    "类似于http://www.hankcs.com/nlp/word2vec.html#h2-3 ，可以提高效率。\n",
    "\n",
    "### 效果与速度\n",
    "\n",
    "效果与最好的神经网络模型相差无几，但训练速度非常快：\n",
    "\n",
    "![006Fmjmcly1fggfltgxwcj30pl0dewgh.jpg](https://i.loli.net/2018/07/17/5b4d9d30a8eca.jpg)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- fastText常常可以跟深度神经网络分类器打平。\n",
    "- 但训练速度只需几秒，而不是几天。\n",
    "- 还可以学习多种语言的词向量（效果比word2vec还要好）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
